import streamlit as st
import google.generativeai as genai
from tavily import TavilyClient
import PyPDF2

# --- 1. Configurare PaginÄƒ ---
st.set_page_config(
    page_title="Marketing Portfolio Optimizer",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# --- 2. Gestionare Secrete (API Keys) ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
except FileNotFoundError:
    st.error("âš ï¸ Cheile API nu sunt configurate! Te rog configureazÄƒ 'GOOGLE_API_KEY' È™i 'TAVILY_API_KEY' Ã®n Streamlit Secrets.")
    st.stop()

# Configurare Clienti API
genai.configure(api_key=GOOGLE_API_KEY)
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

# --- 3. InterfaÈ›a GraficÄƒ (UI) ---
st.title("ğŸ“ˆ Asistent AI: Optimizare Portofoliu PromoÈ›ionale")
st.markdown("""
**Salut!** Sunt asistentul tÄƒu virtual pentru analizÄƒ de produs.
Alege modelul AI potrivit, Ã®ncarcÄƒ catalogul È™i hai sÄƒ optimizÄƒm portofoliul!
""")

# DicÈ›ionar cu modelele disponibile È™i numele lor prietenoase
AVAILABLE_MODELS = {
    "Gemini 1.5 Flash (Rapid & Context Mare)": "gemini-1.5-flash",
    "Gemini 1.5 Pro (InteligenÈ›Äƒ MaximÄƒ)": "gemini-1.5-pro",
    "Gemini 1.0 Pro (Versiunea Standard)": "gemini-1.0-pro"
}

with st.sidebar:
    st.header("âš™ï¸ SetÄƒri AI")
    
    # Selector pentru Model
    selected_model_name = st.selectbox(
        "Alege Modelul AI:",
        list(AVAILABLE_MODELS.keys()),
        index=0, # Default: Flash
        help="Flash este rapid È™i bun pentru documente mari. Pro este mai lent dar oferÄƒ analize mai profunde."
    )
    # Extragem ID-ul tehnic al modelului (ex: 'gemini-1.5-flash')
    model_api_id = AVAILABLE_MODELS[selected_model_name]

    st.divider()
    
    st.header("ğŸ“‚ Documente")
    uploaded_file = st.file_uploader("ÃncarcÄƒ Catalogul (PDF)", type=['pdf'])
    
    st.info(f"Model activ: **{model_api_id}**")
    
    if st.button("È˜terge Istoric Chat"):
        st.session_state.messages = []
        st.rerun()

# --- 4. FuncÈ›ii Backend ---

def extract_text_from_pdf(pdf_file):
    """CiteÈ™te textul din PDF paginÄƒ cu paginÄƒ."""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception as e:
        st.error(f"Eroare la citirea PDF-ului: {e}")
        return None

def search_internet(query):
    """CautÄƒ pe internet folosind Tavily pentru context actualizat."""
    try:
        response = tavily_client.search(
            query=query, 
            search_depth="advanced", 
            max_results=5,
            include_answer=True
        )
        
        context_parts = []
        if 'answer' in response:
            context_parts.append(f"RÄƒspuns direct Tavily: {response['answer']}")
        
        for res in response.get('results', []):
            context_parts.append(f"- {res['content']} (Sursa: {res['url']})")
            
        return "\n".join(context_parts)
    except Exception as e:
        return f"Eroare la cÄƒutarea pe internet: {e}"

# --- 5. Logica PrincipalÄƒ a AplicaÈ›iei ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# Procesare PDF
if uploaded_file:
    if "current_file_name" not in st.session_state or st.session_state.current_file_name != uploaded_file.name:
        with st.spinner("â³ Citesc È™i analizez catalogul..."):
            pdf_text = extract_text_from_pdf(uploaded_file)
            if pdf_text:
                st.session_state.pdf_content = pdf_text
                st.session_state.current_file_name = uploaded_file.name
                st.success(f"âœ… Catalogul '{uploaded_file.name}' a fost procesat! PoÈ›i Ã®ncepe conversaÈ›ia.")
            else:
                st.warning("Nu am putut extrage text din acest PDF.")

# AfiÈ™are istoric
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input Utilizator
if prompt := st.chat_input("Ex: Ce produse eco-friendly sunt Ã®n trend È™i lipsesc din catalogul nostru?"):
    
    if "pdf_content" not in st.session_state:
        st.error("Te rog Ã®ncarcÄƒ mai Ã®ntÃ¢i un catalog PDF Ã®n bara din stÃ¢nga.")
    else:
        # Salvare mesaj user
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Generare RÄƒspuns
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            with st.spinner(f"ğŸ” Caut pe internet È™i analizez cu {selected_model_name}..."):
                
                # a) CÄƒutare Internet
                web_knowledge = search_internet(prompt)
                
                # b) Configurare Model Selectat din ListÄƒ
                model = genai.GenerativeModel(model_api_id)
                
                # c) Prompt
                system_instruction = f"""
                EÈ™ti un Senior Product Manager È™i Marketing Strategist.
                
                MODEL AI FOLOSIT: {selected_model_name}
                
                SARCINA:
                AjutÄƒ echipa de marketing sÄƒ optimizeze portofoliul.
                
                CONTEXT CATALOG (PDF): 
                {st.session_state.pdf_content[:60000]} 
                
                CONTEXT INTERNET:
                {web_knowledge}
                
                INSTRUCÈšIUNI:
                - AnalizeazÄƒ ce avem Ã®n catalog vs ce se cere pe piaÈ›Äƒ.
                - Fii critic dar constructiv.
                - OferÄƒ sugestii concrete.
                """
                
                full_prompt = f"{system_instruction}\n\nÃNTREBAREA UTILIZATORULUI: {prompt}"

                try:
                    response = model.generate_content(full_prompt, stream=True)
                    full_response = ""
                    for chunk in response:
                        if chunk.text:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    message_placeholder.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                
                except Exception as e:
                    message_placeholder.error(f"Eroare generare ({model_api_id}): {e}")
