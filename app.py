import streamlit as st
import google.generativeai as genai
from tavily import TavilyClient
import PyPDF2
import os

# --- 1. Configurare PaginÄƒ ---
st.set_page_config(
    page_title="Marketing Portfolio Optimizer",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# --- 2. Gestionare Secrete (API Keys) ---
# ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm cheile din st.secrets (SetÄƒrile din Streamlit Cloud)
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
except FileNotFoundError:
    st.error("âš ï¸ Cheile API nu sunt configurate! Te rog configureazÄƒ 'GOOGLE_API_KEY' È™i 'TAVILY_API_KEY' Ã®n Streamlit Secrets.")
    st.stop()

# Configurare Clienti API
genai.configure(api_key=GOOGLE_API_KEY)
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

# --- 3. InterfaÈ›a GraficÄƒ (UI) ---
st.title("ğŸ“ˆ Asistent AI: Optimizare Portofoliu PromoÈ›ionale")
st.markdown("""
**Salut!** Sunt asistentul tÄƒu virtual pentru analizÄƒ de produs.
1. ÃncarcÄƒ catalogul PDF curent.
2. ÃntreabÄƒ-mÄƒ orice despre optimizare, trenduri sau comparaÈ›ii cu piaÈ›a.
""")

with st.sidebar:
    st.header("ğŸ“‚ Documente")
    uploaded_file = st.file_uploader("ÃncarcÄƒ Catalogul (PDF)", type=['pdf'])
    
    st.markdown("---")
    st.markdown("**Cum funcÈ›ioneazÄƒ?**")
    st.markdown("1. AI-ul citeÈ™te tot PDF-ul.")
    st.markdown("2. CautÄƒ pe internet informaÈ›ii live despre trenduri.")
    st.markdown("3. ÃÈ›i oferÄƒ sfaturi strategice.")
    
    if st.button("È˜terge Istoric Chat"):
        st.session_state.messages = []
        st.rerun()

# --- 4. FuncÈ›ii Backend ---

def extract_text_from_pdf(pdf_file):
    """CiteÈ™te textul din PDF paginÄƒ cu paginÄƒ."""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception as e:
        st.error(f"Eroare la citirea PDF-ului: {e}")
        return None

def search_internet(query):
    """CautÄƒ pe internet folosind Tavily pentru context actualizat."""
    try:
        # CÄƒutare avansatÄƒ pentru a obÈ›ine conÈ›inut relevant
        response = tavily_client.search(
            query=query, 
            search_depth="advanced", 
            max_results=5,
            include_answer=True
        )
        
        # Construim un rezumat al surselor gÄƒsite
        context_parts = []
        if 'answer' in response:
            context_parts.append(f"RÄƒspuns direct Tavily: {response['answer']}")
        
        for res in response.get('results', []):
            context_parts.append(f"- {res['content']} (Sursa: {res['url']})")
            
        return "\n".join(context_parts)
    except Exception as e:
        return f"Eroare la cÄƒutarea pe internet: {e}"

# --- 5. Logica PrincipalÄƒ a AplicaÈ›iei ---

# IniÈ›ializare istoric chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Procesare PDF (doar cÃ¢nd se Ã®ncarcÄƒ un fiÈ™ier nou)
if uploaded_file:
    # VerificÄƒm dacÄƒ fiÈ™ierul a fost deja procesat ca sÄƒ nu pierdem timp
    if "current_file_name" not in st.session_state or st.session_state.current_file_name != uploaded_file.name:
        with st.spinner("â³ Citesc È™i analizez catalogul... (poate dura cÃ¢teva secunde)"):
            pdf_text = extract_text_from_pdf(uploaded_file)
            if pdf_text:
                st.session_state.pdf_content = pdf_text
                st.session_state.current_file_name = uploaded_file.name
                st.success(f"âœ… Catalogul '{uploaded_file.name}' a fost procesat! PoÈ›i Ã®ncepe conversaÈ›ia.")
            else:
                st.warning("Nu am putut extrage text din acest PDF.")

# AfiÈ™are mesaje anterioare
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Zona de input pentru utilizator
if prompt := st.chat_input("Ex: Ce produse eco-friendly sunt Ã®n trend È™i lipsesc din catalogul nostru?"):
    
    if "pdf_content" not in st.session_state:
        st.error("Te rog Ã®ncarcÄƒ mai Ã®ntÃ¢i un catalog PDF Ã®n bara din stÃ¢nga.")
    else:
        # 1. AdÄƒugÄƒm mesajul utilizatorului Ã®n istoric
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. Procesare RÄƒspuns AI
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            with st.spinner("ğŸ” Caut pe internet È™i compar cu catalogul tÄƒu..."):
                
                # a) Cutare pe internet
                web_knowledge = search_internet(prompt)
                
                # b) Configurare Model AI (Gemini 1.5 Flash)
                model = genai.GenerativeModel('gemini-1.5-flash')
                
                # c) Construire Prompt Complex
                system_instruction = f"""
                EÈ™ti un Senior Product Manager È™i Marketing Strategist pentru o companie de produse promoÈ›ionale.
                
                SARCINA TA:
                AjutÄƒ echipa de marketing sÄƒ optimizeze portofoliul rÄƒspunzÃ¢nd la Ã®ntrebarea utilizatorului.
                
                DATE DISPONIBILE:
                1. CATALOGUL NOSTRU (PDF): 
                {st.session_state.pdf_content[:60000]} 
                *(Text trunchiat pentru optimizare dacÄƒ e prea lung)*
                
                2. INFORMAÈšII EXTERNE (INTERNET - TRENDURI/COMPETIÈšIE):
                {web_knowledge}
                
                INSTRUCÈšIUNI DE RÄ‚SPUNS:
                - AnalizeazÄƒ ce avem Ã®n catalog vs ce se cere pe piaÈ›Äƒ (conform datelor de pe internet).
                - Fii critic dar constructiv. DacÄƒ un produs e demodat, spune-o clar.
                - OferÄƒ sugestii concrete (nume de produse, materiale, culori).
                - RÄƒspunde Ã®n limba RomÃ¢nÄƒ, formatat frumos cu Markdown (bold, liste).
                """
                
                full_prompt = f"{system_instruction}\n\nÃNTREBAREA UTILIZATORULUI: {prompt}"

                try:
                    # Generare rÄƒspuns stream (sÄƒ aparÄƒ textul pe mÄƒsurÄƒ ce e scris)
                    response = model.generate_content(full_prompt, stream=True)
                    full_response = ""
                    for chunk in response:
                        if chunk.text:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    message_placeholder.markdown(full_response)
                    
                    # Salvare Ã®n istoric
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                
                except Exception as e:
                    error_msg = f"A apÄƒrut o eroare la generare: {e}"
                    message_placeholder.error(error_msg)
